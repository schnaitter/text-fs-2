{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e48b545-bdbf-4de3-ab91-071a79f9473a",
   "metadata": {},
   "source": [
    "# üöÄ Quellcode-Analyse einer Website "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7180d-559a-49a0-a197-2a875b87de73",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b> üîî Feinlernziel(e) dieses Kapitels</b></br>\n",
    "   Sie k√∂nnen die Semantik der textangebenden html-Tags beschreiben und beschreiben, welche Tags zur Textextraktion verwendet werden. </br>\n",
    "Sie k√∂nnen den Unterschied zwischen statischen und flexiblen Webinhalten benennen. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0fc85-7ba3-4b30-8996-5abc2ccc8df6",
   "metadata": {},
   "source": [
    "## Hinweise zur Ausf√ºhrung des Notebooks\n",
    "Dieses Notebook kann auf unterschiedlichen Levels erarbeitet werden (siehe Abschnitt [\"Technische Voraussetzungen\"](../markdown/introduction_requirements)): \n",
    "1. Book-Only Mode\n",
    "2. Cloud Mode: Daf√ºr auf üöÄ klicken und z.B. in Colab ausf√ºhren.\n",
    "3. Local Mode: Daf√ºr auf Herunterladen ‚Üì klicken und \".ipynb\" w√§hlen. \n",
    "\n",
    "## √úbersicht\n",
    "Im Folgenden wird exemplarisch der HTML-Code der Website der Senatskanzlei Berlin auf seine Struktur hin untersucht und es wird eine strukturierte Methode zur Inhaltsextraktion entwickelt.\n",
    "\n",
    "Daf√ºr werden folgendene Schritte durchgef√ºhrt:\n",
    "1. Abfragen des HTML-Codes \n",
    "2. Strukturiertes Parsen des HTML-Codes\n",
    "4. Verlinkten Seiten nachgehen und parsen\n",
    "5. Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cee5ec-2f6b-4255-abe6-10bc2bc3bc8b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Informationen zum Ausf√ºhren des Notebooks ‚Äì Zum Ausklappen klicken ‚¨áÔ∏è</b></summary>\n",
    "  \n",
    "<b>Voraussetzungen zur Ausf√ºhrung des Jupyter Notebooks:</b>\n",
    "<ul>\n",
    "<li> Installieren der Bibliotheken </li>\n",
    "</ul>\n",
    "Zum Testen: Ausf√ºhren der Zelle \"load libraries\".</br>\n",
    "Alle Zellen, die mit üöÄ gekennzeichnet sind, werden nur bei der Ausf√ºhrung des Noteboos in Colab / JupyterHub bzw. lokal ausgef√ºhrt. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d3ffe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#  üöÄ Install libraries \n",
    "! pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94ceb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag, Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08044f02",
   "metadata": {},
   "source": [
    "## 1. HTML-Code der Website der Senatskanzlei abfragen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set url to the URL of the senatskanzlei Berlin\n",
    "url = \"https://www.berlin.de/rbmskzl/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform get request, save HTML in the variable response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the get request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Die Abfrage des HTML-Codes war erfolgreich\")\n",
    "else:\n",
    "    print(f\"Die Abfrage war nicht erfolgreich! Der Fehlercode ist: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HTML text from the reponse\n",
    "html_text = response.text\n",
    "\n",
    "# display the first 100 characters of the HTML\n",
    "html_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc7e9b-c400-49a2-a7d8-97ea9fa5a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f554e8",
   "metadata": {},
   "source": [
    "## 2. Strukturiertes Parsen des HTML-Codes\n",
    "1. Aufbau der Seite analysieren und Auswahl treffen\n",
    "2. Top-News-Sektion parsen\n",
    "3. Politische Themen parsen\n",
    "4. Pressemitteilungen abfragen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65718e88-a197-4cca-8fbb-abb20b0da5e0",
   "metadata": {},
   "source": [
    "### 2.1 Aufbau der Seite analysieren\n",
    "![berlin_de_top](berlin_de_top.png)\n",
    "\n",
    "4 Sektionen\n",
    "1. Top-Sektion: A -- k√∂nnte auch (so wie es aussieht) aus 2 Teilen bestehen\n",
    "2. Oft gesucht: B\n",
    "3. Politische Themen: C\n",
    "4. Pressemitteilungen: D\n",
    "\n",
    "Uns interessiert A, C und D\n",
    "\n",
    "#### Struktur von A und C\n",
    "* √úberschrift, Text, Link in \"Weitere Informationen\"\n",
    "\n",
    "#### Struktur von D\n",
    "* Datum, Titel, Link in dem Titel\n",
    "\n",
    "Strategie: \n",
    "1. Ist die visuelle Aufteilung in den Tags abgebildet?\n",
    "  * Ist der Tag, der die Abschnitte unterteilt einmalig f√ºr den jeweiligen Abschnitt?\n",
    "3. Welche Tags (mit Attribut) fassen die einzelnen Teile zusammen?\n",
    "4. Wie sind die Inhalte in den einzelnen Teile strukturiert? Welche hierarchische Gliederung gibt es? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b004b8",
   "metadata": {},
   "source": [
    "### 2.2 A (Top-News-Sektion) parsen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb87e9-e1f5-4297-9333-39452a6a3c1d",
   "metadata": {},
   "source": [
    "#### 2.2.1 Ausfinding machen, ob es einen Tag gibt, dem die Inhalte der Sektion A untergeordnet sind. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e318f9a-8c69-4896-9299-730d3f50ace5",
   "metadata": {},
   "source": [
    "Das ist der Fall f√ºr den div-Container `<div>`  mit CSS class `'herounit-homepage herounit-homepage--default'`.\n",
    "Mit der Library `beautifulsoup` kann der gesamte Container, also alle tags, die dem div-Container untergeordnet sind, extrahiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb944c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tags that are children of the div tag with matching CSS class\n",
    "topdiv = soup.find(\"div\", {\"class\": \"herounit-homepage herounit-homepage--default\"})\n",
    "\n",
    "# print the content of the topdiv\n",
    "print(topdiv.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21e510-ba4e-42bd-ba48-aae960eb60a6",
   "metadata": {},
   "source": [
    "#### 2.2.2 Titel extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f806b62-d548-4bbf-85da-33088bdbfe0c",
   "metadata": {},
   "source": [
    "Wir sehen, dass alle Titel unter `h2`-Tags stehen. Diese k√∂nnen wir im n√§chsten Schritt extrahieren. Wir gehen dabei von dem bereits extrahierten Top-Div aus und extrahieren nur `h2`-Tags, die diesem Tag untergeordnet sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae9455-c5b3-4f97-8f39-2dceb6411375",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_h2titles = topdiv.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37511e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all h2 content that is a child of the top div\n",
    "topdiv_h2titles = topdiv.find_all('h2')\n",
    "\n",
    "# retrieve the content and clean it\n",
    "topdiv_h2titles = [entry.text.strip() for entry in topdiv_h2titles]\n",
    "\n",
    "print(topdiv_h2titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90edfa-586f-4004-871a-d8d9c656d8ce",
   "metadata": {},
   "source": [
    "#### 2.2.3 Kurzbeschreibungen extrahieren "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0733a-85f2-4ef4-9b40-53c75e89692a",
   "metadata": {},
   "source": [
    "Wir sehen weiter, dass alle Kurzbeschreibungen als paragraphs `<p>` ausgezeichnet sind. Im Folgenden extrahieren wir alle Paragraphen und lassen uns das Ergebnis anzeigen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d0d7e-82d7-4b41-89aa-e0ac432cf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all paragraphs that are children of the top div\n",
    "topdiv_texts =  topdiv.find_all('p')\n",
    "topdiv_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e47fd4-cda9-4724-ac95-e4a99212a702",
   "metadata": {},
   "source": [
    "Wir extrahieren zwar so alle Kurzbeschreibungen, unsere Liste beinhaltet allerdings auch die Beschreibung eines Bilds. Da sich das `class`-Attribut der Kurzbeschreibung von dem des Bilds unterscheidet, k√∂nnen wir durch das zus√§tzliche Abgleichen des Attributs eine Liste erstellen, in der nur die Kurzbeschreibungen vorhanden sind: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7ded0-4de6-4883-b6a2-33bcd993531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all short description from the p for which the attribute \"class\" equals \"text\"\n",
    "topdiv_texts =  topdiv.find_all('p', {\"class\":\"text\"})\n",
    "\n",
    "# retrieve the content and clean it\n",
    "topdiv_texts = [entry.text.strip() for entry in topdiv_texts]\n",
    "\n",
    "# print the extracted content\n",
    "topdiv_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362edbba-8d19-4f2d-93eb-6d67d4dc14f4",
   "metadata": {},
   "source": [
    "#### 2.2.4 Links extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe009b2-e89a-482e-a87f-e4d796e7668f",
   "metadata": {},
   "source": [
    "Auf die gleiche Weise k√∂nnen wir alle Hyperlinks, die in `<a>`-Tags gespeichert sind extrahieren. Der Hyperlink selbst steht in dem Attribut `href`, dessen Wert wir gezielt abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19884b36-cc09-4df7-adef-bb96e3c5a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_links =  topdiv.find_all('a')\n",
    "topdiv_links = [entry.get('href') for entry in topdiv_links]\n",
    "\n",
    "# print the extracted links\n",
    "topdiv_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ca3f8-c774-4a8b-9aa4-e24356136112",
   "metadata": {},
   "source": [
    "Wir sehen, dass die Links keine vollst√§ndigen URLs sind, da sie weder mit \"www.\" noch mit \"https://\" anfangen. Diese Links nennen wir **relative URLs**. Sie verweisen auf Unterseiten der aktuellen Seite (die Startseite der Senatskanzlei), die Adresse der Unterseiten wird relativ zur aktuellen Seite angegeben.\n",
    "\n",
    "Um aus den relativen URLs absolte URLs zu erzeugen, die auch durch Einf√ºgen in die Adresszeile eines Browsers abrufbar sind, muss den URLs das Pr√§fix der aktuellen Seite vorangestellt werden, in unserem Fall \"https://www.berlin.de/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc253dff-791d-4f38-bc6b-24696a9e74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create absolute URLs\n",
    "def make_links_absolute(link_list, prefix=\"https://www.berlin.de\"):\n",
    "    absolute_links = []\n",
    "    for link in link_list:\n",
    "        if not link.startswith(\"https\"):\n",
    "            absolute_links.append(prefix + link)\n",
    "        else:\n",
    "            absolute_links.append(link)    \n",
    "    return absolute_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e456837-b465-47cb-9f50-92ce9735e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_links = make_links_absolute(topdiv_links)\n",
    "topdiv_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd340922-a662-4388-8062-0f598b335bbb",
   "metadata": {},
   "source": [
    "#### Zusammenf√ºgen der Daten \n",
    "* alle m√ºssen die gleich L√§nge haben\n",
    "* verlassen uns hier auf die Reihenfolge der Daten in den verschiedenen Listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16c279-f3f9-4ed0-9bff-75b38db2dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topdiv_h2titles) == len(topdiv_texts) == len(topdiv_absolute_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df720ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_section_data = {\"Titel\": topdiv_h2titles,\n",
    "                  \"Text\":topdiv_texts, \n",
    "                  \"URL\":topdiv_absolute_links,\n",
    "                   \"Datum\": datetime.today().strftime('%d.%m.%Y') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_section_data_df = pd.DataFrame(top_section_data)\n",
    "top_section_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4297fc4-9cda-4e6d-bdb0-008a43e45f21",
   "metadata": {},
   "source": [
    "### 2.3 Teil B (Politische Themen) parsen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c97cf-a7bb-485a-b39f-8669f28b5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = soup.findAll(\"section\", {\"class\": \"modul-multiteaser\"})\n",
    "print(len(sections))\n",
    "print(sections[1].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599470d4-8172-4510-9479-10fa4eb5762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_section(section, prefix=\"\"):\n",
    "    content =  {\"Titel\":[], \"Text\":[]}\n",
    "    titles_with_tags = section.find_all('h3')\n",
    "    content[\"Titel\"] = [entry.text.strip() for entry in titles_with_tags]\n",
    "    \n",
    "    section_ps = section.find_all('p', {\"class\":\"text\"})\n",
    "    links = []\n",
    "    for p in section_ps:\n",
    "        content[\"Text\"].append(p.find(string=True).strip())\n",
    "        links.append(p.find('a').get('href'))\n",
    "    content[\"URL\"] = make_links_absolute(links)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bee5c-5531-4508-ad6e-ca9cc809954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "politische_themen = {\"Titel\":[], \"Text\":[], \"URL\":[]}\n",
    "for section in sections:\n",
    "    content = process_single_section(section)\n",
    "    politische_themen.update(content)\n",
    "politische_themen[\"Datum\"] = datetime.today().strftime('%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5516519-31b3-4348-897d-e784d4661ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "politische_themen_df = pd.DataFrame(politische_themen)\n",
    "politische_themen_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ffefe4-893a-4285-b068-72ac3f84fbeb",
   "metadata": {},
   "source": [
    "### 2.4 Pressemitteilungen parsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf8d1e-a04f-4026-b9d4-ce85b459d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressemitteilungen_parsed = {}\n",
    "\n",
    "pressemitteilungen = soup.find(\"ul\", {\"class\":\"list--tablelist ruler has-date\"})\n",
    "pressemitteilungen_parsed[\"Datum\"] = [date.text for date in pressemitteilungen.findAll(\"div\", {\"class\":\"cell date\"})]\n",
    "pressemitteilungen_parsed[\"Text\"] =  [a.text for a in pressemitteilungen.findAll(\"a\")]\n",
    "\n",
    "links = [a.get(\"href\") for a in pressemitteilungen.findAll(\"a\")]\n",
    "pressemitteilungen_parsed[\"URL\"] = make_links_absolute(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf78369-1437-49e2-92c7-f027fa13435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressemitteilungen_parsed_df = pd.DataFrame(pressemitteilungen_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e6e3d1-c1ea-4fd0-a096-3bb77709ba31",
   "metadata": {},
   "source": [
    "### 2.5 Daten der Startseite zusammenf√ºgen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b296e21-3067-40dd-8eb7-d030cc3ee0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "startseite_df = pd.concat([top_section_data_df, politische_themen_df, pressemitteilungen_parsed_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3561908-0bea-492f-9579-ef8197f2a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "startseite_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e377a744-de5f-4264-9619-1ea67c921b5e",
   "metadata": {},
   "source": [
    "## 3. Links zu den Mitteilungen nachgehen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af189b09-75f3-46b5-a1fa-0de783fdc0dc",
   "metadata": {},
   "source": [
    "### 3.1 Extraktion des Volltexts der verlinkten Websites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e380886-1092-41e3-9254-e0fb94e51323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/1936466/how-to-scrape-only-visible-webpage-text-with-beautifulsoup\n",
    "\n",
    "# Only get text that is visible on the website \n",
    "# Inlcudes ads, pointers to more info and so on \n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(soup):\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return [t.strip() for t in visible_texts if len(t) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6a4dd-d858-45a3-b2d7-7fe02511fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = []\n",
    "for url in startseite_df.URL:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_text = response.text\n",
    "        soup = BeautifulSoup(html_text)\n",
    "        text = text_from_html(soup)\n",
    "        full_texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92feaa15-5b6d-4bd2-a1ed-612481b86763",
   "metadata": {},
   "source": [
    "### 3.2 Dirty clean the full texts by setting a length threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee8d44-f6b9-4c1e-8167-d7f73b6e0a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenghts = [len(entry) for entry in full_texts[0]]\n",
    "sorted(lenghts)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed42cfd-81d9-4ca2-a9df-33abf34d0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 100\n",
    "cleaned_full_text = []\n",
    "for entry in full_texts:\n",
    "    cleaned = []\n",
    "    for snippet in entry:\n",
    "        if len(snippet) > threshold:\n",
    "            cleaned.append(snippet)\n",
    "    cleaned_full_text.append(\"\\n\".join(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498620c2-ace4-4bf4-9fb3-611ca27357bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "startseite_df[\"Volltext\"] = cleaned_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821fa42-96d0-455b-87fa-07809ff0fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "startseite_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46e380-0ce0-4672-8380-833f22a61aae",
   "metadata": {},
   "source": [
    "## 4. Ergebnisse speichern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630d25a-8527-44d6-bdb0-367ae36c0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.today().strftime('%Y-%m-&d')\n",
    "startseite_df.to_csv(f\"{date}_Startseite_Senatskanzlei_Berlin.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
